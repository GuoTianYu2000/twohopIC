{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## twohop in-context test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from safetensors.torch import load_file, save_file\n",
    "import os\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "# load meta-llama llama3.1 model using vllm:\n",
    "# we haven't tested the llama3.1-70b model using vllm\n",
    "MODEL_OPTIONS = {\n",
    "    \"qwen\": {\n",
    "        \"name\": \"Qwen/Qwen2.5-7B\",\n",
    "        \"dirname\": \"./qwen2.5\",\n",
    "        \"trust_remote_code\": True\n",
    "    },\n",
    "    \"llama3-8b\": {\n",
    "        \"name\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "        \"dirname\": \"./llama3.1\",\n",
    "        \"trust_remote_code\": False\n",
    "    },\n",
    "    \"llama3-70b\": {\n",
    "        \"name\": \"meta-llama/Meta-Llama-3-70B\",\n",
    "        \"dirname\": \"./llama3.1-70b\",\n",
    "        \"trust_remote_code\": False\n",
    "    },\n",
    "    \"olmo\": {\n",
    "        \"name\": \"allenai/OLMo-7B-hf\",\n",
    "        \"dirname\": \"./olmo\",\n",
    "        \"trust_remote_code\": True\n",
    "    }\n",
    "}\n",
    "model_type = \"llama3-70b\"  # Change this to use different models\n",
    "model_config = MODEL_OPTIONS[model_type]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_config[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-18 13:52:48 config.py:542] This model supports multiple tasks: {'score', 'reward', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 02-18 13:52:48 config.py:1401] Defaulting to use mp for distributed inference\n",
      "INFO 02-18 13:52:48 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='meta-llama/Meta-Llama-3-70B', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-70B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3-70B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 02-18 13:52:49 multiproc_worker_utils.py:300] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 02-18 13:52:49 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2318980)\u001b[0;0m INFO 02-18 13:52:49 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2318983)\u001b[0;0m INFO 02-18 13:52:49 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2318988)\u001b[0;0m INFO 02-18 13:52:49 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2318993)\u001b[0;0m INFO 02-18 13:52:49 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2318998)\u001b[0;0m INFO 02-18 13:52:49 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2319003)\u001b[0;0m INFO 02-18 13:52:49 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2319008)\u001b[0;0m INFO 02-18 13:52:49 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 02-18 13:52:50 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2318998)\u001b[0;0m INFO 02-18 13:52:50 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2318983)\u001b[0;0m INFO 02-18 13:52:50 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2318993)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2318980)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2319003)\u001b[0;0m INFO 02-18 13:52:50 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 02-18 13:52:50 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 02-18 13:52:50 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2318988)\u001b[0;0m INFO 02-18 13:52:50 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2319008)\u001b[0;0m INFO 02-18 13:52:50 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 02-18 13:52:54 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2318988)\u001b[0;0m INFO 02-18 13:52:54 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2318980)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2319008)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2318983)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2318993)\u001b[0;0m INFO 02-18 13:52:54 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2318998)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2319003)\u001b[0;0m INFO 02-18 13:52:54 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 02-18 13:52:54 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 02-18 13:52:54 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 02-18 13:52:54 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 02-18 13:52:54 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2318988)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2318980)\u001b[0;0m INFO 02-18 13:52:54 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2318993)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2318983)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2319008)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2318998)\u001b[0;0m INFO 02-18 13:52:54 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 02-18 13:52:54 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 02-18 13:52:54 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 02-18 13:52:54 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2319003)\u001b[0;0m INFO 02-18 13:52:54 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 02-18 13:52:54 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 02-18 13:52:54 pynccl.py:69] vLLM is using nccl==2.21.5\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Initialize the LLM with the specified model and enable multi-GPU inference\n",
    "llm = LLM(model=model_config[\"name\"], tensor_parallel_size=8)\n",
    "\n",
    "# Prepare the prompt\n",
    "prompt = \"Ben lives in Vesperon. People in Vesperon speak Arabic. Mark lives in Noctari. People in Noctari speak Japanese. Therefore, Ben speaks\"\n",
    "\n",
    "# Prepare the sampling parameters\n",
    "sampling_params = SamplingParams(temperature=1.0)\n",
    "\n",
    "# Run inference using vllm with 8 GPUs\n",
    "outputs = llm.generate([prompt], sampling_params)\n",
    "\n",
    "# The outputs variable now contains the inference results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Initialize the LLM with the specified model\n",
    "llm = LLM(model=model_config[\"name\"])\n",
    "\n",
    "# Prepare the sampling parameters\n",
    "sampling_params = SamplingParams(temperature=0.0, num_samples=100)\n",
    "\n",
    "# Run inference using vllm\n",
    "outputs = llm.generate([prompt], sampling_params)\n",
    "\n",
    "# The outputs variable now contains the inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "\n",
    "dirname = model_config[\"dirname\"]\n",
    "\n",
    "def split_batch(batch: List[Any], batch_size: int) -> List[List[Any]]:\n",
    "    \"\"\"Split a batch into smaller batches of specified size.\"\"\"\n",
    "    return [batch[i:i + batch_size] for i in range(0, len(batch), batch_size)]\n",
    "\n",
    "def process_batch(\n",
    "    inputs: List[str],\n",
    "    tracked_indices: List[List[str]],\n",
    "    llm: Any,\n",
    "    batch_size: int = 32,  # Adjust this based on your GPU memory\n",
    "    maxlength: int = 2048\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process a batch of inputs with dynamic splitting to avoid OOM.\"\"\"\n",
    "    # Split inputs into smaller batches\n",
    "    input_batches = split_batch(inputs, batch_size)\n",
    "    tracked_batches = split_batch(tracked_indices, batch_size)\n",
    "    all_outputs = []\n",
    "\n",
    "    # Process each mini-batch\n",
    "    for input_batch, tracked_batch in zip(input_batches, tracked_batches):\n",
    "        # Prepare the sampling parameters\n",
    "        sampling_params = SamplingParams(max_length=maxlength)\n",
    "        \n",
    "        # Run inference using vllm\n",
    "        outputs = llm.generate(input_batch, sampling_params)\n",
    "        \n",
    "        # Collect outputs\n",
    "        all_outputs.extend(outputs)\n",
    "    \n",
    "    return all_outputs\n",
    "\n",
    "def main():\n",
    "    # Load your dataset\n",
    "    with open(os.path.join(dirname, f\"test_long.json\"), \"r\") as f:\n",
    "        test_long = json.load(f)\n",
    "    \n",
    "    # Set batch size based on your GPU memory\n",
    "    BATCH_SIZE = 200  # Adjust this value based on your GPU memory\n",
    "    \n",
    "    # Initialize the LLM with the specified model\n",
    "    llm = LLM(model=model_config[\"name\"])\n",
    "    \n",
    "    # Process each main batch\n",
    "    for k, input_texts in tqdm(test_long.items()):\n",
    "        # Extract inputs and tracked indices\n",
    "        inputs = [pair[\"question\"] for pair in input_texts]\n",
    "        tracked_indices = [pair[\"query_names\"] + pair[\"non_query_names\"] for pair in input_texts]\n",
    "        # Calculate max length for this mini-batch\n",
    "        lengths = [len(input) for input in inputs]\n",
    "        maxlength = max(lengths)\n",
    "        # Process the batch with dynamic splitting\n",
    "        outputs = process_batch(\n",
    "            inputs=inputs,\n",
    "            tracked_indices=tracked_indices,\n",
    "            llm=llm,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            maxlength=maxlength\n",
    "        )\n",
    "        \n",
    "        # Save outputs\n",
    "        save_name = os.path.join(dirname, f\"outputs_hopk{k}.json\")\n",
    "        with open(save_name, \"w\") as f:\n",
    "            json.dump(outputs, f)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/tmp/ipykernel_2280184/2670755056.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  logits = torch.load(save_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor([0.0008, 0.0056, 0.2346, 0.0019, 0.0047, 0.3462], dtype=torch.float16)\n",
      "2 tensor([0.0020, 0.0024, 0.1029, 0.0350, 0.0187, 0.2581], dtype=torch.float16)\n",
      "2 tensor([0.0010, 0.0090, 0.8213, 0.0162, 0.0117, 0.1375], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:16<00:48, 16.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor([0.0077, 0.0143, 0.2321, 0.2971, 0.1127, 0.2820], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor([0.0004, 0.0056, 0.3811, 0.0089, 0.0119, 0.2993], dtype=torch.float16)\n",
      "2 tensor([0.0006, 0.0060, 0.3562, 0.0094, 0.0179, 0.5210], dtype=torch.float16)\n",
      "2 tensor([0.0086, 0.0310, 0.6411, 0.0114, 0.0248, 0.2484], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:14<00:43, 14.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor([0.0016, 0.0276, 0.1979, 0.3940, 0.1085, 0.2441], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor([7.7969e-06, 7.7969e-06, 7.7969e-06, 7.7969e-06, 7.7969e-06, 7.7969e-06])\n",
      "2 tensor([7.7969e-06, 7.7969e-06, 7.7969e-06, 7.7969e-06, 7.7969e-06, 7.7969e-06])\n",
      "2 tensor([7.7969e-06, 7.7969e-06, 7.7969e-06, 7.7969e-06, 7.7969e-06, 7.7969e-06])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:27<01:22, 27.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor([7.7969e-06, 7.7969e-06, 7.7969e-06, 7.7969e-06, 7.7969e-06, 7.7969e-06])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor([2.3866e-04, 1.4519e-02, 3.3008e-01, 1.5192e-03, 2.2400e-02, 3.1421e-01],\n",
      "       dtype=torch.float16)\n",
      "2 tensor([0.0015, 0.0523, 0.4160, 0.0157, 0.0588, 0.3999], dtype=torch.float16)\n",
      "2 tensor([0.0132, 0.1033, 0.4712, 0.0240, 0.0405, 0.2637], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:07<00:21,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor([0.0109, 0.0980, 0.2064, 0.1420, 0.1320, 0.2454], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def get_tracked_prob(input_texts, logits, tokenizer):\n",
    "    \"\"\"Calculate the tracked probabilities for given input_texts and logits.\"\"\"\n",
    "    inputs = [pair['question'] for pair in input_texts]\n",
    "    tracked_indices = [pair['query_names'] + pair['non_query_names'] for pair in input_texts]\n",
    "    lengths = [len(tokenizer(input).input_ids) for input in inputs]\n",
    "    check_indices = torch.LongTensor([[i, l-1, j] for i, l in enumerate(lengths) for j in tracked_indices[i]])\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    tracked_prob = probs[check_indices[:, 0], check_indices[:, 1], check_indices[:, 2]]\n",
    "    return tracked_prob\n",
    "\n",
    "def split_topics(input_texts):\n",
    "    keywords = [\"locate\", \"grand\", \"family\", \"three\"]\n",
    "    display_keywords = {\n",
    "        \"locate\": \"geography\",\n",
    "        \"grand\": \"relations\",\n",
    "        \"family\": \"biology\",\n",
    "        \"three\": \"arithmetic\",\n",
    "        \"other\": \"other\"\n",
    "    }\n",
    "    topic_dict = {keyword: [] for keyword in keywords}\n",
    "    topic_dict[\"other\"] = []\n",
    "    topic_indices = {keyword: [] for keyword in keywords}\n",
    "    topic_indices[\"other\"] = []\n",
    "\n",
    "    for idx, pair in enumerate(input_texts):\n",
    "        question = pair[\"question\"]\n",
    "        found = False\n",
    "        for keyword in keywords:\n",
    "            if keyword in question:\n",
    "                topic_dict[keyword].append(pair)\n",
    "                topic_indices[keyword].append(idx)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            topic_dict[\"locate\"].append(pair)\n",
    "            topic_indices[\"locate\"].append(idx)\n",
    "    \n",
    "    for topic, texts in topic_dict.items():\n",
    "        yield display_keywords[topic], texts, topic_indices[topic]\n",
    "\n",
    "parent_dir = os.path.dirname(model_config[\"dirname\"])\n",
    "tracked_prob_all = {}\n",
    "\n",
    "for alias, config in MODEL_OPTIONS.items():\n",
    "    dirname = config[\"dirname\"]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config[\"name\"])\n",
    "    test_long_path = os.path.join(dirname, \"test_long.json\")\n",
    "    tracked_prob_all[alias] = {}\n",
    "    \n",
    "    if not os.path.isfile(test_long_path):\n",
    "        continue\n",
    "    \n",
    "    with open(test_long_path, \"r\") as f:\n",
    "        test_long = json.load(f)\n",
    "\n",
    "    for k, input_texts in tqdm(test_long.items()):\n",
    "        if int(k) > 2:\n",
    "            break\n",
    "        save_name = os.path.join(dirname, f\"logits_hopk{k}.pt\")\n",
    "        logits = torch.load(save_name)\n",
    "        for topic, input_text_group, indices in split_topics(input_texts):\n",
    "            if topic == 'other':\n",
    "                continue\n",
    "            tracked_prob = get_tracked_prob(input_text_group, logits[indices, ...], tokenizer)\n",
    "            if k not in tracked_prob_all[alias]:\n",
    "                tracked_prob_all[alias][k] = {}\n",
    "            tracked_prob_all[alias][k][topic] = tracked_prob.view(-1, 3*int(k)).mean(dim=0)\n",
    "            print(k, tracked_prob.view(-1, 3*int(k)).mean(dim=0))\n",
    "\n",
    "torch.save(tracked_prob_all, \"./tracked_prob_all.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twohop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
