{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianyu/miniconda3/envs/twohop/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from transformers import AutoTokenizer\n",
    "import pdb\n",
    "\n",
    "from scipy.stats import normaltest\n",
    "import os\n",
    "# print(os.getcwd())\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# from src.models import get_tokenizer_and_model\n",
    "# from src.modified_forward import ModelWrapper\n",
    "# from src.utils import *\n",
    "# from src.probe_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\"values that you need to change\"\"\"\n",
    "PROJECT_PATH = \"/home/tianyu/TwoHopIC\" # the path of your working directory\n",
    "access_token = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate dataset for finetuning and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 87848.03it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 68860.68it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown model type: llama3-13b",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 168\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Create tracked indices\u001b[39;00m\n\u001b[1;32m    166\u001b[0m all_C_entities \u001b[38;5;241m=\u001b[39m short_names \u001b[38;5;241m+\u001b[39m mixed_locations \u001b[38;5;241m+\u001b[39m mixed_biology \u001b[38;5;241m+\u001b[39m languages\n\u001b[1;32m    167\u001b[0m tracked_indices \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 168\u001b[0m     name: \u001b[43mget_token_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m all_C_entities\n\u001b[1;32m    170\u001b[0m }\n\u001b[1;32m    172\u001b[0m ft_data[model_type] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n",
      "Cell \u001b[0;32mIn[2], line 83\u001b[0m, in \u001b[0;36mget_token_index\u001b[0;34m(tokenizer, name, model_type)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown model type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown model type: llama3-13b"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from transformers import AutoTokenizer\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import normaltest\n",
    "import os\n",
    "import json\n",
    "from torch.nn import functional as F\n",
    "random.seed(42)\n",
    "MODEL_OPTIONS = {\n",
    "    \"qwen\": {\n",
    "        \"name\": \"Qwen/Qwen2.5-7B\",\n",
    "        \"dirname\": os.path.join(PROJECT_PATH, \"qwen2.5\"),\n",
    "        \"trust_remote_code\": True\n",
    "    },\n",
    "    \"llama3-8b\": {\n",
    "        \"name\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "        \"dirname\": os.path.join(PROJECT_PATH, \"llama3.1\"),\n",
    "        \"trust_remote_code\": False\n",
    "    },\n",
    "    \"llama3-13b\": {\n",
    "        \"name\": \"meta-llama/Meta-Llama-3-70B\",\n",
    "        \"dirname\": os.path.join(PROJECT_PATH, \"llama3.1-70b\"),\n",
    "        \"trust_remote_code\": False\n",
    "    },\n",
    "    \"olmo\": {\n",
    "        \"name\": \"allenai/OLMo-7B-hf\",\n",
    "        \"dirname\": os.path.join(PROJECT_PATH, \"olmo\"),\n",
    "        \"trust_remote_code\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Template and entity definitions\n",
    "\n",
    "\n",
    "proto_template = [\n",
    "    \"[A] is the mother of [B]. [B] is the mother of [C]. Therefore, [A] is the grandmother of\",\n",
    "    \"[A] is the father of [B]. [B] is the father of [C]. Therefore, [A] is the grandfather of\",\n",
    "    \"[A] is a city in the state of [B]. The state of [B] is part of the country [C]. Therefore, [A] is located in\",\n",
    "    \"[A] is a species in the genus [B]. The genus [B] belongs to the family [C]. Therefore, [A] is classified under the family\",\n",
    "    \"[A] follows the time zone of [B]. [B] is three hours ahead of [C]. Therefore, [A] is three hours ahead of\",\n",
    "    \"[A] lives in [B]. People in [B] speak [C]. Therefore, [A] speaks\"\n",
    "]\n",
    "\n",
    "mixed_locations = [\"Zorvath\", \"Tyseria\", \"Kryo\", \"Vynora\", \"Quellion\", \"Dras\", \n",
    "                  \"Luminax\", \"Vesperon\", \"Noctari\", \"Xyphodon\", \"Glacidae\", \"Ophirion\",\n",
    "                  \"Eryndor\", \"Solmyra\", \"Umbrithis\", \"Balthorien\", \"Ytheris\", \"Fendrel\", \"Havroth\", \"Marendor\"]\n",
    "\n",
    "mixed_biology = [\"Fluxilus\", \"Varnex\", \"Dranthidae\", \"Zynthor\", \"Gryvus\", \"Myralin\",\n",
    "                \"Thalorium\", \"Zephyra\", \"Aerinth\", \"Xyphodon\", \"Kryostis\", \"Glacidae\",\n",
    "                \"Borithis\", \"Chrysalix\", \"Noctilura\", \"Phorvian\", \"Seraphid\", \"Uthrelin\",\n",
    "                \"Eldrinth\", \"Yvorith\"]\n",
    "\n",
    "languages = [\"English\", \"Spanish\", \"Mandarin\", \"Hindi\", \"Arabic\", \n",
    "            \"French\", \"German\", \"Japanese\", \"Portuguese\", \"Russian\",\n",
    "            \"Korean\", \"Italian\", \"Turkish\", \"Dutch\", \"Swedish\", \n",
    "            \"Polish\", \"Hebrew\", \"Greek\", \"Bengali\", \"Thai\"]\n",
    "\n",
    "short_names = [\"Ben\", \"Jack\", \"Luke\", \"Mark\", \"Paul\", \"John\", \"Tom\", \n",
    "              \"Sam\", \"Joe\", \"Max\", \"Amy\", \"Emma\", \"Anna\", \"Grace\", \n",
    "              \"Kate\", \"Lucy\", \"Sarah\", \"Alice\", \"Alex\", \"Ruby\"]\n",
    "\n",
    "# Token handling functions\n",
    "def get_token_index(tokenizer, name, model_type):\n",
    "    \"\"\"\n",
    "    Get the correct token index based on model type, handling BOS tokens appropriately\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(' ' + name)\n",
    "    \n",
    "    if model_type in [\"llama3-8b\", \"llama3-70b\"]:\n",
    "        return tokens[1]  # Skip BOS token\n",
    "    elif model_type == \"olmo\":\n",
    "        return tokens[0]\n",
    "    elif model_type == \"qwen\":\n",
    "        return tokens[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "def process(sentence, category, names):\n",
    "    \"\"\"\n",
    "    Replaces the placeholders [A], [B], [C] in `sentence` with the given list of `names`.\n",
    "    \"\"\"\n",
    "    return (sentence\n",
    "            .replace(\"[A]\", names[0])\n",
    "            .replace(\"[B]\", names[1])\n",
    "            .replace(\"[C]\", names[2])\n",
    "           )\n",
    "\n",
    "def pick_category_and_names(template_str):\n",
    "    \"\"\"\n",
    "    Return category from template_str.\n",
    "    \"\"\"\n",
    "    if \"speak\" in template_str:\n",
    "        return \"language\"\n",
    "    elif \"city\" in template_str or \"located\" in template_str or \"time zone\" in template_str:\n",
    "        return \"geography\"\n",
    "    elif \"species\" in template_str or \"genus\" in template_str:\n",
    "        return \"biology\"\n",
    "    else:\n",
    "        return \"human\"\n",
    "\n",
    "def sample_names(category):\n",
    "    \"\"\"\n",
    "    Returns one triple of names depending on the category.\n",
    "    \"\"\"\n",
    "    if category == \"language\":\n",
    "        return [\n",
    "            random.choice(short_names),\n",
    "            random.choice(mixed_locations),\n",
    "            random.choice(languages)\n",
    "        ]\n",
    "    elif category == \"geography\":\n",
    "        return random.sample(mixed_locations, 3)\n",
    "    elif category == \"biology\":\n",
    "        return random.sample(mixed_biology, 3)\n",
    "    else:\n",
    "        return random.sample(short_names, 3)\n",
    "\n",
    "def sample_k_disjoint_sets(category, k):\n",
    "    \"\"\"\n",
    "    Returns a list of k disjoint name-triples for the given category.\n",
    "    \"\"\"\n",
    "    sets = []\n",
    "    if category == \"language\":\n",
    "        sn_candidates = random.sample(short_names, k)\n",
    "        loc_candidates = random.sample(mixed_locations, k)\n",
    "        lang_candidates = random.sample(languages, k)\n",
    "        for i in range(k):\n",
    "            sets.append([sn_candidates[i], loc_candidates[i], lang_candidates[i]])\n",
    "    \n",
    "    elif category == \"geography\":\n",
    "        loc_sample = random.sample(mixed_locations, 3*k)\n",
    "        for i in range(k):\n",
    "            sets.append(loc_sample[3*i : 3*i+3])\n",
    "    \n",
    "    elif category == \"biology\":\n",
    "        bio_sample = random.sample(mixed_biology, 3*k)\n",
    "        for i in range(k):\n",
    "            sets.append(bio_sample[3*i : 3*i+3])\n",
    "    \n",
    "    else:  # human relationship\n",
    "        sn_sample = random.sample(short_names, 3*k)\n",
    "        for i in range(k):\n",
    "            sets.append(sn_sample[3*i : 3*i+3])\n",
    "    \n",
    "    return sets\n",
    "\n",
    "# Main data generation loop\n",
    "ft_data = {}\n",
    "\n",
    "for model_type, model_config in MODEL_OPTIONS.items():\n",
    "    # Load tokenizer for the current model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_config[\"name\"],\n",
    "        trust_remote_code=model_config[\"trust_remote_code\"],\n",
    "        timeout=10 \n",
    "    )\n",
    "    \n",
    "    # Create tracked indices\n",
    "    all_C_entities = short_names + mixed_locations + mixed_biology + languages\n",
    "    tracked_indices = {\n",
    "        name: get_token_index(tokenizer, name, model_type) \n",
    "        for name in all_C_entities\n",
    "    }\n",
    "    \n",
    "    ft_data[model_type] = {}\n",
    "    \n",
    "    for k in range(1, 2):\n",
    "        ft_data[model_type][k] = []\n",
    "        for _ in tqdm(range(1000)):\n",
    "            # Pick template and category\n",
    "            temp = random.choice(proto_template)\n",
    "            category = pick_category_and_names(temp)\n",
    "            \n",
    "            # Split template and sample names\n",
    "            sentences = temp.split(\". \")\n",
    "            k_name_sets = sample_k_disjoint_sets(category, k)\n",
    "            \n",
    "            # Create deques with processed sentences\n",
    "            all_deques = []\n",
    "            for names_i in k_name_sets:\n",
    "                dq_i = deque([process(s, category, names_i) for s in sentences])\n",
    "                all_deques.append((dq_i, names_i))\n",
    "            \n",
    "            # Generate text by popping from deques\n",
    "            text = \"\"\n",
    "            while any(len(dq_tuple[0]) > 1 for dq_tuple in all_deques):\n",
    "                indices_lengths = [\n",
    "                    (idx, len(dq) - 1) \n",
    "                    for idx, (dq, _) in enumerate(all_deques) \n",
    "                    if len(dq) > 1\n",
    "                ]\n",
    "                \n",
    "                total_len_minus_1 = sum(x[1] for x in indices_lengths)\n",
    "                if total_len_minus_1 == 0:\n",
    "                    break\n",
    "                \n",
    "                # Weighted random choice\n",
    "                rand_val = random.random()\n",
    "                cumulative = 0.0\n",
    "                chosen_idx = None\n",
    "                for (idx_deq, l_minus_1) in indices_lengths:\n",
    "                    frac = l_minus_1 / total_len_minus_1\n",
    "                    if rand_val < cumulative + frac:\n",
    "                        chosen_idx = idx_deq\n",
    "                        break\n",
    "                    cumulative += frac\n",
    "                \n",
    "                if chosen_idx is not None:\n",
    "                    chosen_dq, _ = all_deques[chosen_idx]\n",
    "                    text += chosen_dq.popleft() + \". \"\n",
    "            \n",
    "            # Handle final query\n",
    "            query_idx = random.randint(0, k - 1)\n",
    "            query_dq, query_names = all_deques[query_idx]\n",
    "            \n",
    "            if len(query_dq) > 0:\n",
    "                text += query_dq.popleft()\n",
    "            \n",
    "            # Get answer and token indices\n",
    "            ans = \" \" + query_names[-1]\n",
    "            query_names_ids = [tracked_indices[n] for n in query_names]\n",
    "            non_query_names_ids = [\n",
    "                tracked_indices[n] \n",
    "                for idx, (_, names) in enumerate(all_deques) \n",
    "                if idx != query_idx \n",
    "                for n in names\n",
    "            ]\n",
    "            \n",
    "            # Save result\n",
    "            ft_data[model_type][k].append({\n",
    "                'question': text,\n",
    "                'answer': ans,\n",
    "                'query_names': query_names_ids,\n",
    "                'non_query_names': non_query_names_ids,\n",
    "            })\n",
    "\n",
    "    with open(os.path.join(model_config[\"dirname\"], f\"test_short.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(ft_data[model_type], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twohop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
