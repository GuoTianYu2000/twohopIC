{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## twohop in-context test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/project/twohopIC/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from transformers import AutoTokenizer\n",
    "import pdb\n",
    "import json\n",
    "from scipy.stats import normaltest\n",
    "import os\n",
    "# print(os.getcwd())\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from transformers import AutoTokenizer\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "from src.llm_eval_configs import MODEL_OPTIONS, proto_template, short_names, mixed_locations, mixed_biology, languages\n",
    "\n",
    "from scipy.stats import normaltest\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\"values that you need to change\"\"\"\n",
    "PROJECT_PATH = \".\" # the path of your working directory\n",
    "access_token = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ends(values):\n",
    "    k = 1\n",
    "    end_probs = []\n",
    "    while 3*k - 1 < len(values):\n",
    "        end_probs.append(values[3*k-1])\n",
    "        k += 1\n",
    "    summarize_probs = [end_probs[0].item(), torch.mean(torch.tensor(end_probs[1:])).item()]\n",
    "    return summarize_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = {}\n",
    "summaries = {}\n",
    "for model, model_config in MODEL_OPTIONS.items():\n",
    "    probs[model] = torch.load(os.path.join(model_config[\"dirname\"], \"tracked_prob.pt\"))\n",
    "    summaries[model] = {}\n",
    "    for i in range(1, 6):\n",
    "        summaries[model][i] = (extract_ends(probs[model][str(i)]['overall']['mean']), extract_ends(probs[model][str(i)]['overall']['std_error']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qwen': {1: ([0.8017578125, nan], [0.00525665283203125, nan]),\n",
       "  2: ([0.289794921875, 0.273193359375],\n",
       "   [0.0092010498046875, 0.00751495361328125]),\n",
       "  3: ([0.260498046875, 0.10418701171875],\n",
       "   [0.00839996337890625, 0.003387451171875]),\n",
       "  4: ([0.2578125, 0.064208984375],\n",
       "   [0.00839996337890625, 0.002399444580078125]),\n",
       "  5: ([0.239013671875, 0.04974365234375],\n",
       "   [0.007778167724609375, 0.0020847320556640625])},\n",
       " 'llama3-8b': {1: ([0.900390625, nan], [0.0022487640380859375, nan]),\n",
       "  2: ([0.398193359375, 0.36962890625],\n",
       "   [0.007781982421875, 0.0068206787109375]),\n",
       "  3: ([0.318115234375, 0.19482421875],\n",
       "   [0.0073089599609375, 0.004718780517578125]),\n",
       "  4: ([0.28564453125, 0.1290283203125],\n",
       "   [0.007328033447265625, 0.0033817291259765625]),\n",
       "  5: ([0.268798828125, 0.1004638671875],\n",
       "   [0.007297515869140625, 0.002719879150390625])},\n",
       " 'llama3-70b': {1: ([0.87255859375, nan], [0.005290985107421875, nan]),\n",
       "  2: ([0.4833984375, 0.171875], [0.0069427490234375, 0.0038661956787109375]),\n",
       "  3: ([0.453369140625, 0.100830078125],\n",
       "   [0.007091522216796875, 0.0027294158935546875]),\n",
       "  4: ([0.466796875, 0.06011962890625],\n",
       "   [0.0070037841796875, 0.0021038055419921875]),\n",
       "  5: ([0.486572265625, 0.0457763671875],\n",
       "   [0.00736236572265625, 0.0016689300537109375])},\n",
       " 'olmo': {1: ([0.7197265625, nan], [0.00823211669921875, nan]),\n",
       "  2: ([0.36767578125, 0.319091796875],\n",
       "   [0.007598876953125, 0.006656646728515625]),\n",
       "  3: ([0.2464599609375, 0.1947021484375],\n",
       "   [0.00643157958984375, 0.005401611328125]),\n",
       "  4: ([0.179931640625, 0.1405029296875],\n",
       "   [0.005649566650390625, 0.004383087158203125]),\n",
       "  5: ([0.1409912109375, 0.108642578125],\n",
       "   [0.004802703857421875, 0.003753662109375])},\n",
       " 'llama2-7b': {1: ([0.81591796875, nan], [0.005611419677734375, nan]),\n",
       "  2: ([0.406005859375, 0.343994140625],\n",
       "   [0.007358551025390625, 0.00666046142578125]),\n",
       "  3: ([0.3095703125, 0.2001953125],\n",
       "   [0.006687164306640625, 0.005100250244140625]),\n",
       "  4: ([0.2490234375, 0.1397705078125],\n",
       "   [0.006290435791015625, 0.00397491455078125]),\n",
       "  5: ([0.195068359375, 0.105712890625],\n",
       "   [0.005706787109375, 0.00327301025390625])}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = {}\n",
    "summaries_lora = {}\n",
    "for model, model_config in MODEL_OPTIONS.items():\n",
    "    try:\n",
    "        probs[model] = torch.load(os.path.join(model_config[\"dirname\"], \"tracked_prob_lora.pt\"))\n",
    "        summaries_lora[model] = {}\n",
    "        for i in range(1, 6):\n",
    "            summaries_lora[model][i] = (extract_ends(probs[model][str(i)]['overall']['mean']), extract_ends(probs[model][str(i)]['overall']['std_error']))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qwen': {1: ([1.0, nan], [1.0728836059570312e-06, nan]),\n",
       "  2: ([1.0, 0.0020008087158203125],\n",
       "   [1.0728836059570312e-06, 0.0014133453369140625]),\n",
       "  3: ([0.99658203125, 0.005157470703125],\n",
       "   [0.0017547607421875, 0.0022411346435546875]),\n",
       "  4: ([0.9951171875, 0.003566741943359375],\n",
       "   [0.002056121826171875, 0.0018062591552734375]),\n",
       "  5: ([0.990234375, 0.0041961669921875],\n",
       "   [0.002902984619140625, 0.0019350051879882812])},\n",
       " 'llama3-8b': {1: ([1.0, nan], [2.2649765014648438e-06, nan]),\n",
       "  2: ([1.0, 0.0020008087158203125],\n",
       "   [4.76837158203125e-07, 0.0014133453369140625]),\n",
       "  3: ([0.99755859375, 0.0041046142578125],\n",
       "   [0.001316070556640625, 0.00196075439453125]),\n",
       "  4: ([0.9970703125, 0.003299713134765625],\n",
       "   [0.0016508102416992188, 0.0017652511596679688]),\n",
       "  5: ([0.99609375, 0.00469970703125],\n",
       "   [0.0018815994262695312, 0.0020694732666015625])},\n",
       " 'olmo': {1: ([1.0, nan], [2.2172927856445312e-05, nan]),\n",
       "  2: ([1.0, 5.960464477539063e-08],\n",
       "   [5.900859832763672e-06, 5.960464477539063e-08]),\n",
       "  3: ([0.66357421875, 0.16796875], [0.01432037353515625, 0.01142120361328125]),\n",
       "  4: ([0.56640625, 0.1444091796875],\n",
       "   [0.01476287841796875, 0.01020050048828125]),\n",
       "  5: ([0.499755859375, 0.1241455078125],\n",
       "   [0.01454925537109375, 0.009185791015625])},\n",
       " 'llama2-7b': {1: ([1.0, nan], [7.152557373046875e-07, nan]),\n",
       "  2: ([1.0, 0.0010023117065429688],\n",
       "   [1.4901161193847656e-06, 0.00099945068359375]),\n",
       "  3: ([0.92041015625, 0.037322998046875],\n",
       "   [0.0081939697265625, 0.005725860595703125]),\n",
       "  4: ([0.8916015625, 0.0304412841796875],\n",
       "   [0.00933074951171875, 0.005123138427734375]),\n",
       "  5: ([0.8623046875, 0.0284576416015625],\n",
       "   [0.01042938232421875, 0.00498199462890625])}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_lora"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
